{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-22T03:47:53.564429Z",
     "start_time": "2024-08-22T03:47:48.295059Z"
    }
   },
   "source": [
    "import json\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "split = \"test\"\n",
    "rewrite_method = \"slimplmqr\"\n",
    "dataset = \"hotpot-qa\"\n",
    "version=\"v0715\"\n",
    "search_engine=\"bing\"\n",
    "\n",
    "node_file = f\"../html_data/{dataset}/treegen/v0715/{search_engine}html-{rewrite_method}-v0715-{dataset}-{split}.jsonl\"\n",
    "node_lines = [json.loads(line) for line in open(node_file)]\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T03:47:54.603059Z",
     "start_time": "2024-08-22T03:47:54.402405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "idx = 1\n",
    "print(node_lines[idx].keys())\n",
    "# dict_keys(['qa_pairs', 'wikipages', 'annotations', 'id', 'question', 'answer', 'short_answers', 'long_answers', 'html', 'paths', 'path_token_ids', 'node_tree', 'path_divisible'])\n",
    "paths = node_lines[idx]['paths']\n",
    "print(paths[0])\n",
    "# ['html0', 'title']\n",
    "node_tree = node_lines[idx]['node_tree']\n",
    "print(node_tree[0])\n",
    "\n",
    "html= node_lines[idx]['html']\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "parent=soup\n",
    "for tag in paths[0]:\n",
    "    for c in parent.contents:\n",
    "        if c.name == tag:\n",
    "            parent = c\n",
    "            break\n",
    "            \n",
    "print(parent.get_text())"
   ],
   "id": "a2cedc933f4215c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'output', 'question', 'answer', 'html', 'paths', 'path_token_ids', 'node_tree', 'path_divisible'])\n",
      "['title']\n",
      "-1|0.0|[]\n",
      "l'Oiseau Blanc (\"White Bird\")\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T03:58:36.055058Z",
     "start_time": "2024-08-22T03:58:35.770168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "chat_tokenizer_path = \"../../../huggingface/Meta-Llama-3.1-8B-Instruct\"\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(chat_tokenizer_path, trust_remote_code=True)"
   ],
   "id": "589cbe4fe4bf2abe",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T05:51:59.903609Z",
     "start_time": "2024-08-22T05:51:58.631485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from html4rag.html_utils import *\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "url=\"http://172.16.0.96/\"\n",
    "query_instruction_for_retrieval = \"Represent this sentence for searching relevant passages: \"\n",
    "\n",
    "\n",
    "embedder = TEIEmbeddings(\n",
    "            model=url,\n",
    "            huggingfacehub_api_token=\"a-default-token\",\n",
    "            model_kwargs={\"truncate\": True})\n",
    "\n",
    "nidx = 1\n",
    "node_docs=[]\n",
    "paths = node_lines[nidx]['paths']\n",
    "html= node_lines[nidx]['html']\n",
    "path_divisible = node_lines[nidx]['path_divisible']\n",
    "question = query_instruction_for_retrieval + node_lines[nidx]['question']\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "for pidx in range(len(paths)):\n",
    "    parent=soup\n",
    "    for tag in paths[pidx]:\n",
    "        for c in parent.contents:\n",
    "            if c.name == tag:\n",
    "                parent = c\n",
    "                break\n",
    "    node_docs.append(Document(page_content=parent.get_text(), metadata={\"path_idx\": pidx}))\n",
    "batch_size = 16\n",
    "db=FAISS.from_documents(node_docs[:batch_size], embedder)\n",
    "for doc_batch_idx in range(batch_size, len(node_docs), batch_size):\n",
    "    db.add_documents(node_docs[doc_batch_idx:doc_batch_idx+batch_size])\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": len(node_docs)})\n",
    "ranked_docs=retriever.invoke(question)\n",
    "path_rankings = [doc.metadata[\"path_idx\"] for doc in ranked_docs]\n",
    "\n"
   ],
   "id": "1532cfe2057000f1",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T05:52:01.173083Z",
     "start_time": "2024-08-22T05:52:00.603043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from html4rag.html_utils import simplify_html, truncate_input, trim_path\n",
    "#  trim html according to the path_rankings\n",
    "import bs4\n",
    "context_window = \"2k\"\n",
    "max_context_window = int(context_window[:-1]) * 1000\n",
    "             \n",
    "        \n",
    "paths = [{\"path\": paths[i], \"divisible\": path_divisible[i]} for i in range(len(paths))]   \n",
    "for idj in range(len(paths)):\n",
    "    path_idx = int(path_rankings[idj])\n",
    "    paths[path_idx][\"ranking\"] = idj\n",
    "    \n",
    "soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "for idj in range(len(paths)):\n",
    "    path = paths[idj][\"path\"]\n",
    "    tag = soup\n",
    "    for p in path:\n",
    "        for child in tag.contents:\n",
    "            if isinstance(child, bs4.element.Tag):\n",
    "                if child.name == p:\n",
    "                    tag = child\n",
    "                    break\n",
    "\n",
    "    paths[idj][\"tag\"] = tag\n",
    "    paths[idj][\"token_length\"] = len(chat_tokenizer.encode(str(tag), add_special_tokens=False))\n",
    "    if paths[idj][\"token_length\"] < 64:\n",
    "        #  move paths that are too short to the end\n",
    "        paths[idj][\"ranking\"] = len(paths)\n",
    "#  sort paths by ranking\n",
    "paths = sorted(paths, key=lambda x: x[\"ranking\"])\n",
    "total_token_length = sum([p[\"token_length\"] for p in paths])\n",
    "\n",
    "#  remove low ranking paths\n",
    "while total_token_length > max_context_window:\n",
    "    if len(paths) == 1:\n",
    "        break\n",
    "    discarded_path = paths.pop()\n",
    "    total_token_length -= discarded_path[\"token_length\"]\n",
    "    trim_path(discarded_path)\n",
    "\n",
    "total_token_length = len(chat_tokenizer.encode(simplify_html(soup), add_special_tokens=False))\n",
    "while total_token_length > max_context_window:\n",
    "    if len(paths) == 1:\n",
    "        break\n",
    "    discarded_path = paths.pop()\n",
    "    trim_path(discarded_path)\n",
    "    total_token_length = len(chat_tokenizer.encode(simplify_html(soup), add_special_tokens=False))\n",
    "\n",
    "if total_token_length > max_context_window:\n",
    "    # loguru.logger.warning(f\"dataset {dataset} sample {idx} cannot be trimmed to {max_context_window} tokens\")\n",
    "    html_trim = truncate_input(simplify_html(soup),chat_tokenizer, max_context_window)\n",
    "else:\n",
    "    html_trim = simplify_html(soup)\n",
    "\n",
    "assert len(chat_tokenizer.encode(\n",
    "    html_trim,\n",
    "    add_special_tokens=False)) <= max_context_window, f\"html length: {len(chat_tokenizer.encode(html_trim, add_special_tokens=False))}, max_context_window: {max_context_window}\""
   ],
   "id": "c00a3fe55f2c16f2",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:31:15.149507Z",
     "start_time": "2024-08-23T10:31:11.660043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import threading\n",
    "#  chunk rerank and tree rerank\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from html4rag.html_utils import split_tree\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "split = \"test\"\n",
    "rewrite_method = \"slimplmqr\"\n",
    "dataset = \"hotpot-qa\"\n",
    "version=\"v0715\"\n",
    "search_engine=\"bing\"\n",
    "rerank_model = \"bgelargeen\"\n",
    "\n",
    "context_window = \"2k\"\n",
    "chat_tokenizer_name = \"llama\"\n",
    "fine_trim_ratio = \"1/2\"\n",
    "if fine_trim_ratio == \"1/2\":\n",
    "    coarse_context_window = {\"2k\": \"4k\", \"4k\": \"8k\", \"8k\": \"16k\", \"16k\": \"32k\"}[context_window]\n",
    "elif fine_trim_ratio == \"2/3\":\n",
    "    coarse_context_window = {\"2k\": \"3k\", \"4k\": \"6k\", \"8k\": \"12k\", \"16k\": \"24k\"}[context_window]\n",
    "else:\n",
    "    raise ValueError(f\"fine_trim_ratio {fine_trim_ratio} not supported\")\n",
    "data_file = f\"../html_data/{dataset}/chunk-rerank/{chat_tokenizer_name}/{search_engine}html-{rewrite_method}-{rerank_model}-{dataset}-{split}-{coarse_context_window}.jsonl\"\n",
    "data_lines = [json.loads(line) for line in open(data_file)]\n",
    "data_lines = data_lines[:10]\n",
    "total_len=len(data_lines)\n",
    "\n",
    "thread_pool = []\n",
    "\n",
    "pbar = tqdm(total=total_len)\n",
    "parallel_size = 8\n",
    "def start_thread(rank):\n",
    "    while len(data_lines) > 0:\n",
    "        try:\n",
    "            idx = total_len - len(data_lines)\n",
    "            data_line = data_lines.pop(0)\n",
    "            question = data_line['question']\n",
    "            coarse_html_trim = data_lines[idx][\"html_trim\"]\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "for i in range(parallel_size):\n",
    "    thread = threading.Thread(target=start_thread, args=(i,))\n",
    "    thread.start()\n",
    "    thread_pool.append(thread)\n",
    "\n",
    "for thread in thread_pool:\n",
    "    thread.join()\n",
    "\n",
    "pbar.close()\n",
    "\n"
   ],
   "id": "48398fa709b31ec9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 860.23it/s]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T10:01:24.429796Z",
     "start_time": "2024-08-22T10:01:24.425127Z"
    }
   },
   "cell_type": "code",
   "source": "len(paths)",
   "id": "6855dcd41117a786",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T08:39:44.815723Z",
     "start_time": "2024-08-23T08:39:44.683734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "for dataset in [\"asqa\", \"hotpot-qa\", \"nq\", \"trivia-qa\", \"musique\"]:\n",
    "    dir_name=f\"../html_data/{dataset}/treegen/\"\n",
    "    #  rename treegen to tree-gen\n",
    "    os.rename(dir_name, dir_name.replace(\"treegen\", \"tree-gen\"))"
   ],
   "id": "dadb4f9c55b03547",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fb3824e52dc55bbf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
